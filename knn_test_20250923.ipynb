{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c8efb8",
   "metadata": {},
   "source": [
    "This notebook creates reproducible train/test FileCollections for parent-level k-NN experiments.\n",
    "It uses a dataclass to embed a descriptive, structured provenance into each FileCollection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4a6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # KNN Evaluation Pipeline â€” Abstract Integration\n",
    "# This notebook demonstrates how to use the new abstract classes\n",
    "# (SplitSelectionStrategy, NeighborFilterStrategy, LabelingStrategy)\n",
    "# to manage reproducible, pluggable KNN evaluation runs.\n",
    "\n",
    "# %%\n",
    "import random\n",
    "import logging\n",
    "from collections import Counter\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import exists, and_, func\n",
    "from db import get_db_engine\n",
    "from db.models import File, FileCollection, FilingTag, FileCollectionMember, FileTagLabel, FileContent\n",
    "from knn.evaluation import (\n",
    "    SplitSelectionStrategy,\n",
    "    NeighborFilterStrategy,\n",
    "    LabelingStrategy,\n",
    "    KNNRun,\n",
    ")\n",
    "from knn.evaluation import KNNCollectionProvenance  # updated import path\n",
    "from typing import List, Iterable, Set\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"knn.evaluation.run\")\n",
    "\n",
    "# constants (for demo)\n",
    "EMBED_COL = \"minilm_emb\"\n",
    "TRAIN_RATIO = 0.8\n",
    "RANDOM_SEED = 42\n",
    "PARENTS = [\"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "PER_CHILD_CAP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d88895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_all_descendants(session: Session, root_label: str) -> List[FilingTag]:\n",
    "    descendants, queue = [], [root_label]\n",
    "    seen: Set[str] = set([root_label])\n",
    "    while queue:\n",
    "        parent = queue.pop(0)\n",
    "        kids = session.query(FilingTag).filter(FilingTag.parent_label == parent).all()\n",
    "        for k in kids:\n",
    "            if k.label not in seen:\n",
    "                seen.add(k.label)\n",
    "                descendants.append(k)\n",
    "                queue.append(k.label)\n",
    "    return descendants\n",
    "\n",
    "def files_with_tag_and_embedding(session: Session, tag_label: str) -> List[File]:\n",
    "    emb_col = getattr(FileContent, EMBED_COL)\n",
    "    q = (\n",
    "        session.query(File)\n",
    "        .join(File.content)\n",
    "        .filter(emb_col.isnot(None))\n",
    "        .filter(\n",
    "            exists().where(\n",
    "                and_(FileTagLabel.file_id == File.id, FileTagLabel.tag == tag_label)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return q.all()\n",
    "\n",
    "def create_or_get_collection(session, name: str, prov: KNNCollectionProvenance, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Retrieve or create a FileCollection with the given name.\n",
    "    If it already exists and has members, returns it unchanged.\n",
    "    If it exists but is empty or overwrite=True, updates its metadata and description.\n",
    "    \"\"\"\n",
    "    from db.models import FileCollection, FileCollectionMember\n",
    "\n",
    "    collection = session.query(FileCollection).filter(FileCollection.name == name).first()\n",
    "\n",
    "    if collection:\n",
    "        # Check if it already has members\n",
    "        has_members = (\n",
    "            session.query(FileCollectionMember)\n",
    "            .filter(FileCollectionMember.collection_id == collection.id)\n",
    "            .first()\n",
    "            is not None\n",
    "        )\n",
    "\n",
    "        if has_members and not overwrite:\n",
    "            logger.info(f\"Reusing existing FileCollection '{name}' (id={collection.id}) with existing members.\")\n",
    "            return collection\n",
    "\n",
    "        # Optional: update metadata if explicitly overwriting\n",
    "        if overwrite:\n",
    "            logger.info(f\"Overwriting existing FileCollection '{name}' (id={collection.id}) metadata.\")\n",
    "            collection.description = prov.to_description()\n",
    "            collection.meta = prov.to_metadata()\n",
    "            session.commit()\n",
    "        return collection\n",
    "\n",
    "    # ---- Create a new one ----\n",
    "    new_coll = FileCollection(\n",
    "        name=name,\n",
    "        description=prov.to_description(),\n",
    "        meta=prov.to_metadata()\n",
    "    )\n",
    "    session.add(new_coll)\n",
    "    session.commit()\n",
    "    logger.info(f\"Created new FileCollection '{name}' (id={new_coll.id}).\")\n",
    "    return new_coll\n",
    "\n",
    "def add_members(session: Session, collection: FileCollection, files: Iterable[File], role: str, batch_size: int = 1000) -> int:\n",
    "    \"\"\"\n",
    "    Insert (collection_id, file_id, role) rows safely:\n",
    "      - De-dupes incoming files by file_id\n",
    "      - Skips rows that already exist in DB for this collection+role\n",
    "      - Inserts in batches to avoid giant statements\n",
    "    \"\"\"\n",
    "    # 1) de-dupe input\n",
    "    unique_by_id = {}\n",
    "    for f in files:\n",
    "        if f and getattr(f, \"id\", None) is not None:\n",
    "            unique_by_id.setdefault(f.id, f)\n",
    "    candidates = list(unique_by_id.values())\n",
    "    if not candidates:\n",
    "        logger.info(f\"No candidates to add for {collection.name} ({role}).\")\n",
    "        return 0\n",
    "\n",
    "    # 2) fetch existing file_ids for this collection+role\n",
    "    existing_ids = {\n",
    "        fid for (fid,) in session.query(FileCollectionMember.file_id)\n",
    "        .filter(\n",
    "            FileCollectionMember.collection_id == collection.id,\n",
    "            FileCollectionMember.role == role\n",
    "        ).all()\n",
    "    }\n",
    "\n",
    "    # 3) choose new rows only\n",
    "    to_insert = [f for f in candidates if f.id not in existing_ids]\n",
    "    if not to_insert:\n",
    "        logger.info(f\"All {role} members already present in {collection.name}.\")\n",
    "        return 0\n",
    "\n",
    "    # 4) batch insert\n",
    "    added = 0\n",
    "    for i in range(0, len(to_insert), batch_size):\n",
    "        chunk = to_insert[i:i+batch_size]\n",
    "        session.add_all([\n",
    "            FileCollectionMember(collection_id=collection.id, file_id=f.id, role=role)\n",
    "            for f in chunk\n",
    "        ])\n",
    "        session.commit()\n",
    "        added += len(chunk)\n",
    "\n",
    "    logger.info(f\"Added {added} {role} files to {collection.name}\")\n",
    "    return added\n",
    "\n",
    "# unsure if I'll bother using this function\n",
    "def refresh_collection_counts(session, collection: FileCollection):\n",
    "    from db.models import FileCollectionMember\n",
    "    counts = (\n",
    "        session.query(FileCollectionMember.role, func.count(FileCollectionMember.file_id))\n",
    "        .filter(FileCollectionMember.collection_id == collection.id)\n",
    "        .group_by(FileCollectionMember.role)\n",
    "        .all()\n",
    "    )\n",
    "    counts_dict = {role: count for role, count in counts}\n",
    "    meta = collection.metadata or {}\n",
    "    if \"counts\" not in meta:\n",
    "        meta[\"counts\"] = {}\n",
    "    meta[\"counts\"].update(counts_dict)\n",
    "    collection.metadata = meta\n",
    "    session.commit()\n",
    "\n",
    "\n",
    "class RandomStratifiedSplit(SplitSelectionStrategy):\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"random_stratified_split\"\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return \"Randomly sample up to PER_CHILD_CAP files per child tag, split 80/20 into global train/test sets, ensuring disjointness.\"\n",
    "\n",
    "    def __init__(self, parents, per_child_cap, train_ratio, random_seed=None):\n",
    "        self.parents = parents\n",
    "        self.per_child_cap = per_child_cap\n",
    "        self.train_ratio = train_ratio\n",
    "        self.random_seed = random_seed or 42\n",
    "\n",
    "    def select_files(self, session: Session):\n",
    "        random.seed(self.random_seed)\n",
    "        train_ids, test_ids = [], []\n",
    "        for parent in self.parents:\n",
    "            for child in list_all_descendants(session, parent):\n",
    "                files = files_with_tag_and_embedding(session, child.label)\n",
    "                if not files:\n",
    "                    continue\n",
    "                cap = min(self.per_child_cap, len(files))\n",
    "                sampled = random.sample(files, cap) if len(files) > cap else list(files)\n",
    "                random.shuffle(sampled)\n",
    "                split_idx = int(len(sampled) * self.train_ratio)\n",
    "                train_ids += [f.id for f in sampled[:split_idx]]\n",
    "                test_ids += [f.id for f in sampled[split_idx:]]\n",
    "\n",
    "        # Deduplicate and enforce disjointness\n",
    "        train_ids = list(dict.fromkeys(train_ids))\n",
    "        test_ids = [fid for fid in dict.fromkeys(test_ids) if fid not in set(train_ids)]\n",
    "        return train_ids, test_ids\n",
    "    \n",
    "class NoOpNeighborFilter(NeighborFilterStrategy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"no_op_filter\"\n",
    "    @property\n",
    "    def description(self):\n",
    "        return \"Does not filter any neighbors; uses all candidates.\"\n",
    "    \n",
    "    def filter(self, candidate_ids, test_file_hash = None):\n",
    "        return candidate_ids\n",
    "    \n",
    "class MajorityVoteLabeler(LabelingStrategy):\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"majority_vote\"\n",
    "    \n",
    "    @property\n",
    "    def description(self):\n",
    "        return \"Infers label by majority vote among top-k neighbors.\"\n",
    "    \n",
    "    def infer_label(self, neighbor_tags, neighbor_scores):\n",
    "        if not neighbor_tags:\n",
    "            return None\n",
    "            \n",
    "        tag_counts = Counter(neighbor_tags)\n",
    "        most_common_items = tag_counts.most_common(1)\n",
    "        most_common_tag, _ = most_common_items[0]\n",
    "        \n",
    "        return most_common_tag\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bab0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_collections_from_strategy(session: Session, strategy: SplitSelectionStrategy):\n",
    "    \"\"\"\n",
    "    Generic builder that uses a SplitSelectionStrategy to populate train/test FileCollections.\n",
    "    The strategy name and description are stored in metadata.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Running split strategy: {strategy.name}\")\n",
    "    train_ids, test_ids = strategy.select_files(session)\n",
    "\n",
    "    # Use the base class method with all required parameters\n",
    "    prov = strategy.assemble_provenance_info(\n",
    "        parents=PARENTS,\n",
    "        embedding_col=EMBED_COL,\n",
    "        split_ratio=TRAIN_RATIO,\n",
    "        per_child_cap=PER_CHILD_CAP,\n",
    "        random_seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    train_coll = create_or_get_collection(session=session,\n",
    "                                          name=f\"{prov.split_strategy}_{prov.embedding_column}_train\",\n",
    "                                          prov=prov)\n",
    "    test_coll  = create_or_get_collection(session=session,\n",
    "                                          name=f\"{prov.split_strategy}_{prov.embedding_column}_test\",\n",
    "                                          prov=prov)\n",
    "\n",
    "    # Get File objects\n",
    "    train_files = session.query(File).filter(File.id.in_(train_ids)).all()\n",
    "    test_files  = session.query(File).filter(File.id.in_(test_ids)).all()\n",
    "\n",
    "    add_members(session, train_coll, train_files, role=\"train\")\n",
    "    add_members(session, test_coll, test_files, role=\"test\")\n",
    "\n",
    "    logger.info(f\"Train: {len(train_files)}, Test: {len(test_files)}\")\n",
    "\n",
    "    return train_coll, test_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a093218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 22:00:23,633 db.db INFO Creating database engine\n",
      "2025-10-08 22:00:25,420 knn.evaluation.run INFO Running split strategy: random_stratified_split\n",
      "2025-10-08 22:00:30,096 knn.evaluation.run INFO Created new FileCollection 'random_stratified_split_minilm_emb_train' (id=137).\n",
      "2025-10-08 22:00:30,109 knn.evaluation.run INFO Created new FileCollection 'random_stratified_split_minilm_emb_test' (id=138).\n",
      "2025-10-08 22:00:33,067 knn.evaluation.run INFO Added 2280 train files to random_stratified_split_minilm_emb_train\n",
      "2025-10-08 22:00:34,131 knn.evaluation.run INFO Added 564 test files to random_stratified_split_minilm_emb_test\n",
      "2025-10-08 22:00:34,132 knn.evaluation.run INFO Train: 2280, Test: 564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNNRun created: baseline_random_split\n",
      "Split: random_stratified_split, Filter: no_op_filter, Labeler: majority_vote\n"
     ]
    }
   ],
   "source": [
    "engine = get_db_engine()\n",
    "with Session(engine) as session:\n",
    "    split_strategy = RandomStratifiedSplit(\n",
    "        parents=PARENTS,\n",
    "        per_child_cap=PER_CHILD_CAP,\n",
    "        train_ratio=TRAIN_RATIO,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )\n",
    "    neighbor_filter = NoOpNeighborFilter()\n",
    "    labeler = MajorityVoteLabeler()\n",
    "\n",
    "    train_coll, test_coll = build_collections_from_strategy(session, split_strategy)\n",
    "\n",
    "    # Extract collection names while session is active\n",
    "    train_collection_name = train_coll.name\n",
    "    test_collection_name = test_coll.name\n",
    "\n",
    "    knn_run = KNNRun(\n",
    "        k=5,\n",
    "        name=\"baseline_random_split\",\n",
    "        description=\"Baseline KNN evaluation using random stratified split and majority voting.\",\n",
    "        training_collection=train_coll,\n",
    "        test_collection=test_coll,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"KNNRun created: {knn_run.name}\")\n",
    "    logger.info(f\"Split: {split_strategy.name}, Filter: {neighbor_filter.name}, Labeler: {labeler.name}\")\n",
    "    logger.info(f\"Train collection: {train_collection_name}\")\n",
    "    logger.info(f\"Test collection: {test_collection_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54e367",
   "metadata": {},
   "source": [
    "The code from this point down is for getting knn data for the collections. And designed to possibley be run independently of the Cells above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44329887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sqlalchemy.orm import Session\n",
    "from datetime import datetime\n",
    "from db.db import get_db_engine\n",
    "from db.models import FileCollection, FileCollectionMember, File, FileContent, FileTagLabel\n",
    "from knn.base import cosine_similarity_batch\n",
    "\n",
    "\n",
    "def get_embeddings_and_tags(session: Session, collection_name: str, embed_col: str = \"minilm_emb\"):\n",
    "    \"\"\"\n",
    "    Return dict with:\n",
    "      - \"valid\": list of {file_id, file_hash, embedding, tags}\n",
    "      - \"missing_content\": list of file_hash with no FileContent\n",
    "      - \"missing_tags\": list of file_hash with no FileTagLabel\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve all file IDs and hashes in the collection\n",
    "    collection_files = (\n",
    "        session.query(File.id, File.file_hash)\n",
    "        .join(FileCollectionMember, File.id == FileCollectionMember.file_id)\n",
    "        .join(FileCollection, FileCollectionMember.collection_id == FileCollection.id)\n",
    "        .filter(FileCollection.name == collection_name)\n",
    "        .all()\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    missing_content = []\n",
    "    missing_tags = []\n",
    "\n",
    "    for fid, fhash in collection_files:\n",
    "        # get embedding\n",
    "        content = session.query(FileContent).filter(FileContent.file_hash == fhash).first()\n",
    "        if not content or getattr(content, embed_col) is None:\n",
    "            missing_content.append(fhash)\n",
    "            continue\n",
    "\n",
    "        # get tags\n",
    "        tag_rows = session.query(FileTagLabel.tag).filter(FileTagLabel.file_id == fid).all()\n",
    "        if not tag_rows:\n",
    "            missing_tags.append(fhash)\n",
    "            continue\n",
    "\n",
    "        tags = {t for (t,) in tag_rows}\n",
    "        # infer parent tag if possible (e.g., \"F5\" -> \"F\")\n",
    "        for tag in list(tags):\n",
    "            if len(tag) > 1 and tag[0].isalpha():\n",
    "                tags.add(tag[0])\n",
    "\n",
    "        results.append({\n",
    "            \"file_id\": fid,\n",
    "            \"file_hash\": fhash,\n",
    "            \"embedding\": getattr(content, embed_col),\n",
    "            \"tags\": sorted(tags)\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"valid\": results,\n",
    "        \"missing_content\": missing_content,\n",
    "        \"missing_tags\": missing_tags\n",
    "    }\n",
    "\n",
    "def cosine_knn_neighbors(train_valid: List[dict], test_valid: List[dict], k: int = 5):\n",
    "    \"\"\"\n",
    "    Given valid train/test embedding-tag sets, compute top-k nearest neighbors for each test file.\n",
    "    Returns list of neighbor info dicts.\n",
    "    \"\"\"\n",
    "    train_vecs = np.stack([np.array(f[\"embedding\"]) for f in train_valid])\n",
    "    train_hashes = [f[\"file_hash\"] for f in train_valid]\n",
    "    train_tags = [f[\"tags\"] for f in train_valid]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for test_file in test_valid:\n",
    "        test_vec = np.array(test_file[\"embedding\"])\n",
    "        sims = cosine_similarity_batch(test_vec, train_vecs)\n",
    "        topk_idx = np.argsort(sims)[-k:][::-1]\n",
    "\n",
    "        neighbors = [\n",
    "            {\n",
    "                \"neighbor_hash\": train_hashes[i],\n",
    "                \"neighbor_tags\": train_tags[i],\n",
    "                \"similarity\": float(sims[i]),\n",
    "            }\n",
    "            for i in topk_idx\n",
    "        ]\n",
    "\n",
    "        results.append({\n",
    "            \"test_file_hash\": test_file[\"file_hash\"],\n",
    "            \"true_tags\": test_file[\"tags\"],\n",
    "            \"neighbors\": neighbors\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_knn_evaluation(train_collection, test_collection, k: int = 5, embed_col: str = \"minilm_emb\", output_path=None):\n",
    "    \"\"\"\n",
    "    Executes KNN neighbor computation for a given train/test collection pair.\n",
    "    Returns a dict containing:\n",
    "      - knn_results: list of neighbor data per test file\n",
    "      - missing_content / missing_tags summaries\n",
    "    Optionally saves results to JSON if output_path is given.\n",
    "    \"\"\"\n",
    "    engine = get_db_engine()\n",
    "    with Session(engine) as session:\n",
    "        train_data = get_embeddings_and_tags(session, train_collection, embed_col)\n",
    "        test_data  = get_embeddings_and_tags(session, test_collection, embed_col)\n",
    "\n",
    "    knn_results = cosine_knn_neighbors(train_data[\"valid\"], test_data[\"valid\"], k=k)\n",
    "\n",
    "    summary = {\n",
    "        \"train_collection\": train_collection,\n",
    "        \"test_collection\": test_collection,\n",
    "        \"embedding_column\": embed_col,\n",
    "        \"k\": k,\n",
    "        \"results\": knn_results,\n",
    "        \"missing\": {\n",
    "            \"train\": {\n",
    "                \"content\": train_data[\"missing_content\"],\n",
    "                \"tags\": train_data[\"missing_tags\"]\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"content\": test_data[\"missing_content\"],\n",
    "                \"tags\": test_data[\"missing_tags\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if output_path:\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        print(f\"Saved KNN results to {output_path}\")\n",
    "\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9dfcafd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_collection_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run KNN evaluation using the collection names we extracted\u001b[39;00m\n\u001b[32m      2\u001b[39m timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m results = run_knn_evaluation(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_collection=\u001b[43mtrain_collection_name\u001b[49m,\n\u001b[32m      5\u001b[39m     test_collection=test_collection_name,\n\u001b[32m      6\u001b[39m     k=\u001b[32m5\u001b[39m,\n\u001b[32m      7\u001b[39m     embed_col=EMBED_COL,\n\u001b[32m      8\u001b[39m     output_path=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mknn_results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'train_collection_name' is not defined"
     ]
    }
   ],
   "source": [
    "# Run KNN evaluation using the collection names we extracted\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results = run_knn_evaluation(\n",
    "    train_collection=train_collection_name,\n",
    "    test_collection=test_collection_name,\n",
    "    k=5,\n",
    "    embed_col=EMBED_COL,\n",
    "    output_path=f\"knn_results_{timestamp}.json\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
