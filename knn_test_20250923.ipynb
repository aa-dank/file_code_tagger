{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c8efb8",
   "metadata": {},
   "source": [
    "This notebook creates reproducible train/test FileCollections for parent-level k-NN experiments.\n",
    "It uses a dataclass to embed a descriptive, structured provenance into each FileCollection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "import logging, random\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Dict, Any, Iterable, Set\n",
    "\n",
    "from sqlalchemy import and_, exists\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from db import get_db_engine\n",
    "from db.models import (\n",
    "    FilingTag, File, FileContent, FileCollection, FileCollectionMember, FileTagLabel\n",
    ")\n",
    "from knn.knn import CollectionProvenance\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"knn.simple_split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1bc6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Configuration\n",
    "# Adjust these knobs before running the build.\n",
    "\n",
    "# %%\n",
    "PARENTS = [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "EMBED_COL = \"minilm_emb\"\n",
    "TRAIN_RATIO = 0.8\n",
    "PER_CHILD_CAP = 50\n",
    "USE_ALL_DESCENDANTS = True # True: children + grandchildren; False: direct children only\n",
    "RANDOM_SEED = 42 # set None for non-reproducible\n",
    "FILETYPE_FILTER: Optional[List[str]] = [] \n",
    "\n",
    "PURPOSE_TEXT = \"Simple global train/test split for parent-level k-NN experiments.\"\n",
    "STRATEGY_TEXT = \"stratified-by-child-capped\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## DB helpers\n",
    "\n",
    "# %%\n",
    "def list_all_descendants(session: Session, root_label: str) -> List[FilingTag]:\n",
    "    descendants, queue = [], [root_label]\n",
    "    seen: Set[str] = set([root_label])\n",
    "    while queue:\n",
    "        parent = queue.pop(0)\n",
    "        kids = session.query(FilingTag).filter(FilingTag.parent_label == parent).all()\n",
    "        for k in kids:\n",
    "            if k.label not in seen:\n",
    "                seen.add(k.label)\n",
    "                descendants.append(k)\n",
    "                queue.append(k.label)\n",
    "    return descendants\n",
    "\n",
    "def files_with_tag_and_embedding(session: Session, tag_label: str) -> List[File]:\n",
    "    emb_col = getattr(FileContent, EMBED_COL)\n",
    "    q = (\n",
    "        session.query(File)\n",
    "        .join(File.content)\n",
    "        .filter(emb_col.isnot(None))\n",
    "        .filter(\n",
    "            exists().where(\n",
    "                and_(FileTagLabel.file_id == File.id, FileTagLabel.tag == tag_label)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    if FILETYPE_FILTER:\n",
    "        lowered = [ext.lower().lstrip(\".\") for ext in FILETYPE_FILTER]\n",
    "        q = q.filter(File.extension.in_(lowered))\n",
    "    return q.all()\n",
    "\n",
    "def create_or_get_collection(session: Session, name: str, prov: CollectionProvenance) -> FileCollection:\n",
    "    coll = session.query(FileCollection).filter(FileCollection.name == name).first()\n",
    "    if not coll:\n",
    "        coll = FileCollection(name=name, description=prov.to_description(), meta=prov.to_metadata())\n",
    "        session.add(coll); session.commit()\n",
    "        logger.info(f\"Created FileCollection {name}\")\n",
    "    return coll\n",
    "\n",
    "def add_members(session: Session, collection: FileCollection, files: Iterable[File], role: str, batch_size: int = 1000) -> int:\n",
    "    \"\"\"\n",
    "    Insert (collection_id, file_id, role) rows safely:\n",
    "      - De-dupes incoming files by file_id\n",
    "      - Skips rows that already exist in DB for this collection+role\n",
    "      - Inserts in batches to avoid giant statements\n",
    "    \"\"\"\n",
    "    # 1) de-dupe input\n",
    "    unique_by_id = {}\n",
    "    for f in files:\n",
    "        if f and getattr(f, \"id\", None) is not None:\n",
    "            unique_by_id.setdefault(f.id, f)\n",
    "    candidates = list(unique_by_id.values())\n",
    "    if not candidates:\n",
    "        logger.info(f\"No candidates to add for {collection.name} ({role}).\")\n",
    "        return 0\n",
    "\n",
    "    # 2) fetch existing file_ids for this collection+role\n",
    "    existing_ids = {\n",
    "        fid for (fid,) in session.query(FileCollectionMember.file_id)\n",
    "        .filter(\n",
    "            FileCollectionMember.collection_id == collection.id,\n",
    "            FileCollectionMember.role == role\n",
    "        ).all()\n",
    "    }\n",
    "\n",
    "    # 3) choose new rows only\n",
    "    to_insert = [f for f in candidates if f.id not in existing_ids]\n",
    "    if not to_insert:\n",
    "        logger.info(f\"All {role} members already present in {collection.name}.\")\n",
    "        return 0\n",
    "\n",
    "    # 4) batch insert\n",
    "    added = 0\n",
    "    for i in range(0, len(to_insert), batch_size):\n",
    "        chunk = to_insert[i:i+batch_size]\n",
    "        session.add_all([\n",
    "            FileCollectionMember(collection_id=collection.id, file_id=f.id, role=role)\n",
    "            for f in chunk\n",
    "        ])\n",
    "        session.commit()\n",
    "        added += len(chunk)\n",
    "\n",
    "    logger.info(f\"Added {added} {role} files to {collection.name}\")\n",
    "    return added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def build_simple_split():\n",
    "    if RANDOM_SEED is not None:\n",
    "        random.seed(RANDOM_SEED)\n",
    "\n",
    "    engine = get_db_engine()\n",
    "    with Session(engine) as session:\n",
    "        prov = CollectionProvenance(\n",
    "            purpose=PURPOSE_TEXT,\n",
    "            parents=PARENTS,\n",
    "            strategy=STRATEGY_TEXT,\n",
    "            embedding_column=EMBED_COL,\n",
    "            split_ratio=TRAIN_RATIO,\n",
    "            per_child_cap=PER_CHILD_CAP,\n",
    "            filetype_filter=FILETYPE_FILTER,\n",
    "            include_descendants=USE_ALL_DESCENDANTS,\n",
    "            random_seed=RANDOM_SEED,\n",
    "        )\n",
    "\n",
    "        train_coll = create_or_get_collection(session, \"global_train\", prov)\n",
    "        test_coll  = create_or_get_collection(session, \"global_test\", prov)\n",
    "\n",
    "        # We'll accumulate by id to keep things clean\n",
    "        train_files, test_files = [], []\n",
    "\n",
    "        def direct_children(sess, parent):\n",
    "            return sess.query(FilingTag).filter(FilingTag.parent_label == parent).all()\n",
    "\n",
    "        for parent in PARENTS:\n",
    "            children = list_all_descendants(session, parent) if USE_ALL_DESCENDANTS else direct_children(session, parent)\n",
    "\n",
    "            for child in children:\n",
    "                files = files_with_tag_and_embedding(session, child.label)\n",
    "                if not files:\n",
    "                    continue\n",
    "\n",
    "                cap = min(PER_CHILD_CAP, len(files))\n",
    "                sampled = random.sample(files, cap) if len(files) > cap else list(files)\n",
    "                random.shuffle(sampled)\n",
    "\n",
    "                split_idx = int(len(sampled) * TRAIN_RATIO)\n",
    "                train_files.extend(sampled[:split_idx])\n",
    "                test_files.extend(sampled[split_idx:])\n",
    "\n",
    "        # ---- De-dup & disjointness enforcement ----\n",
    "        def dedupe(files):\n",
    "            seen = set()\n",
    "            out = []\n",
    "            for f in files:\n",
    "                if f.id not in seen:\n",
    "                    seen.add(f.id)\n",
    "                    out.append(f)\n",
    "            return out\n",
    "\n",
    "        train_files = dedupe(train_files)\n",
    "        test_files  = dedupe(test_files)\n",
    "\n",
    "        # remove overlaps (prefer keeping in train)\n",
    "        train_ids = {f.id for f in train_files}\n",
    "        test_files = [f for f in test_files if f.id not in train_ids]\n",
    "\n",
    "        logger.info(f\"Prepared train {len(train_files)} / test {len(test_files)} (after dedupe & disjointness)\")\n",
    "\n",
    "        # insert (with extra safety checks inside add_members)\n",
    "        add_members(session, train_coll, train_files, role=\"train\")\n",
    "        add_members(session, test_coll,  test_files,  role=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a093218",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_simple_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54e367",
   "metadata": {},
   "source": [
    "The code from this point down is for getting and analyzing knn performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44329887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adankert\\projects\\file_code_tagger\\utils.py:60: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  r\"N:\\PPDO\\Records\"  (Windows)  or  \"/mnt/records\" (Linux).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sqlalchemy.orm import Session\n",
    "from db.db import get_db_engine\n",
    "from db.models import FileCollection, FileCollectionMember, File, FileContent, FileTagLabel\n",
    "from utils import cosine_similarity_batch\n",
    "\n",
    "def get_embeddings_and_tags(session: Session, collection_name: str, embed_col: str = \"minilm_emb\"):\n",
    "    \"\"\"Return (file_id, hash, embedding, tag) tuples for all files in a collection.\"\"\"\n",
    "    emb_col = getattr(FileContent, embed_col)\n",
    "    q = (\n",
    "        session.query(File.id, File.hash, emb_col, FileTagLabel.tag)\n",
    "        .join(FileCollectionMember, File.id == FileCollectionMember.file_id)\n",
    "        .join(FileCollection, FileCollectionMember.collection_id == FileCollection.id)\n",
    "        .join(FileContent, File.hash == FileContent.file_hash)\n",
    "        .join(FileTagLabel, File.id == FileTagLabel.file_id)\n",
    "        .filter(FileCollection.name == collection_name)\n",
    "    )\n",
    "    return q.all()\n",
    "\n",
    "def evaluate_knn(k: int = 5, embed_col: str = \"minilm_emb\"):\n",
    "    engine = get_db_engine()\n",
    "    with Session(engine) as session:\n",
    "        train = get_embeddings_and_tags(session, \"global_train\", embed_col)\n",
    "        test = get_embeddings_and_tags(session, \"global_test\", embed_col)\n",
    "\n",
    "    # convert to arrays for speed\n",
    "    train_vecs = np.stack([np.array(t[2]) for t in train])\n",
    "    train_tags = [t[3] for t in train]\n",
    "    #train_ids  = [t[0] for t in train]\n",
    "\n",
    "    results = []\n",
    "    for fid, fhash, femb, true_tag in test:\n",
    "        femb = np.array(femb)\n",
    "        sims = cosine_similarity_batch(femb, train_vecs)  # shape (N_train,)\n",
    "        topk_idx = np.argsort(sims)[-k:][::-1]  # top k highest cosine similarities\n",
    "        topk_tags = [train_tags[i] for i in topk_idx]\n",
    "        topk_scores = [float(sims[i]) for i in topk_idx]\n",
    "        pred_tag = max(set(topk_tags), key=topk_tags.count)  # majority vote\n",
    "        correct = pred_tag == true_tag\n",
    "        tag_counts = {t: topk_tags.count(t) for t in set(topk_tags)}\n",
    "\n",
    "        results.append({\n",
    "            \"test_file_id\": fid,\n",
    "            \"test_hash\": fhash,\n",
    "            \"true_tag\": true_tag,\n",
    "            \"topk_tags\": topk_tags,\n",
    "            \"topk_scores\": topk_scores,\n",
    "            \"pred_tag\": pred_tag,\n",
    "            \"is_correct\": correct,\n",
    "            \"neighbor_tag_counts\": tag_counts\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dfcafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:16:49,673 db.db INFO Creating database engine\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_knn(k=5)\n",
    "\n",
    "#store in json file\n",
    "with open(\"knn_results.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
