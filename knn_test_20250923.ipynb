{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c8efb8",
   "metadata": {},
   "source": [
    "This notebook creates reproducible train/test FileCollections for parent-level k-NN experiments.\n",
    "It uses a dataclass to embed a descriptive, structured provenance into each FileCollection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # KNN Evaluation Pipeline â€” Abstract Integration\n",
    "# This notebook demonstrates how to use the new abstract classes\n",
    "# (SplitSelectionStrategy, NeighborFilterStrategy, LabelingStrategy)\n",
    "# to manage reproducible, pluggable KNN evaluation runs.\n",
    "\n",
    "# %%\n",
    "import random\n",
    "import logging\n",
    "from collections import Counter\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import exists, and_\n",
    "from db import get_db_engine\n",
    "from db.models import File, FileCollection, FilingTag, FileCollectionMember, FileTagLabel, FileContent\n",
    "from knn.evaluation import (\n",
    "    SplitSelectionStrategy,\n",
    "    NeighborFilterStrategy,\n",
    "    LabelingStrategy,\n",
    "    KNNRun,\n",
    ")\n",
    "from knn.base import KNNCollectionProvenance  # updated import path\n",
    "from typing import List, Iterable, Set\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"knn.evaluation.run\")\n",
    "\n",
    "# constants (for demo)\n",
    "EMBED_COL = \"minilm_emb\"\n",
    "TRAIN_RATIO = 0.8\n",
    "RANDOM_SEED = 42\n",
    "PARENTS = [\"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "PER_CHILD_CAP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_all_descendants(session: Session, root_label: str) -> List[FilingTag]:\n",
    "    descendants, queue = [], [root_label]\n",
    "    seen: Set[str] = set([root_label])\n",
    "    while queue:\n",
    "        parent = queue.pop(0)\n",
    "        kids = session.query(FilingTag).filter(FilingTag.parent_label == parent).all()\n",
    "        for k in kids:\n",
    "            if k.label not in seen:\n",
    "                seen.add(k.label)\n",
    "                descendants.append(k)\n",
    "                queue.append(k.label)\n",
    "    return descendants\n",
    "\n",
    "def files_with_tag_and_embedding(session: Session, tag_label: str) -> List[File]:\n",
    "    emb_col = getattr(FileContent, EMBED_COL)\n",
    "    q = (\n",
    "        session.query(File)\n",
    "        .join(File.content)\n",
    "        .filter(emb_col.isnot(None))\n",
    "        .filter(\n",
    "            exists().where(\n",
    "                and_(FileTagLabel.file_id == File.id, FileTagLabel.tag == tag_label)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return q.all()\n",
    "\n",
    "def create_or_get_collection(session, name: str, prov: KNNCollectionProvenance, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Retrieve or create a FileCollection with the given name.\n",
    "    If it already exists and has members, returns it unchanged.\n",
    "    If it exists but is empty or overwrite=True, updates its metadata and description.\n",
    "    \"\"\"\n",
    "    from db.models import FileCollection, FileCollectionMember\n",
    "\n",
    "    collection = session.query(FileCollection).filter(FileCollection.name == name).first()\n",
    "\n",
    "    if collection:\n",
    "        # Check if it already has members\n",
    "        has_members = (\n",
    "            session.query(FileCollectionMember)\n",
    "            .filter(FileCollectionMember.collection_id == collection.id)\n",
    "            .first()\n",
    "            is not None\n",
    "        )\n",
    "\n",
    "        if has_members and not overwrite:\n",
    "            logger.info(f\"Reusing existing FileCollection '{name}' (id={collection.id}) with existing members.\")\n",
    "            return collection\n",
    "\n",
    "        # Optional: update metadata if explicitly overwriting\n",
    "        if overwrite:\n",
    "            logger.info(f\"Overwriting existing FileCollection '{name}' (id={collection.id}) metadata.\")\n",
    "            collection.description = prov.to_description()\n",
    "            collection.meta = prov.to_metadata()\n",
    "            session.commit()\n",
    "        return collection\n",
    "\n",
    "    # ---- Create a new one ----\n",
    "    new_coll = FileCollection(\n",
    "        name=name,\n",
    "        description=prov.to_description(),\n",
    "        meta=prov.to_metadata()\n",
    "    )\n",
    "    session.add(new_coll)\n",
    "    session.commit()\n",
    "    logger.info(f\"Created new FileCollection '{name}' (id={new_coll.id}).\")\n",
    "    return new_coll\n",
    "\n",
    "def add_members(session: Session, collection: FileCollection, files: Iterable[File], role: str, batch_size: int = 1000) -> int:\n",
    "    \"\"\"\n",
    "    Insert (collection_id, file_id, role) rows safely:\n",
    "      - De-dupes incoming files by file_id\n",
    "      - Skips rows that already exist in DB for this collection+role\n",
    "      - Inserts in batches to avoid giant statements\n",
    "    \"\"\"\n",
    "    # 1) de-dupe input\n",
    "    unique_by_id = {}\n",
    "    for f in files:\n",
    "        if f and getattr(f, \"id\", None) is not None:\n",
    "            unique_by_id.setdefault(f.id, f)\n",
    "    candidates = list(unique_by_id.values())\n",
    "    if not candidates:\n",
    "        logger.info(f\"No candidates to add for {collection.name} ({role}).\")\n",
    "        return 0\n",
    "\n",
    "    # 2) fetch existing file_ids for this collection+role\n",
    "    existing_ids = {\n",
    "        fid for (fid,) in session.query(FileCollectionMember.file_id)\n",
    "        .filter(\n",
    "            FileCollectionMember.collection_id == collection.id,\n",
    "            FileCollectionMember.role == role\n",
    "        ).all()\n",
    "    }\n",
    "\n",
    "    # 3) choose new rows only\n",
    "    to_insert = [f for f in candidates if f.id not in existing_ids]\n",
    "    if not to_insert:\n",
    "        logger.info(f\"All {role} members already present in {collection.name}.\")\n",
    "        return 0\n",
    "\n",
    "    # 4) batch insert\n",
    "    added = 0\n",
    "    for i in range(0, len(to_insert), batch_size):\n",
    "        chunk = to_insert[i:i+batch_size]\n",
    "        session.add_all([\n",
    "            FileCollectionMember(collection_id=collection.id, file_id=f.id, role=role)\n",
    "            for f in chunk\n",
    "        ])\n",
    "        session.commit()\n",
    "        added += len(chunk)\n",
    "\n",
    "    logger.info(f\"Added {added} {role} files to {collection.name}\")\n",
    "    return added\n",
    "\n",
    "\n",
    "class RandomStratifiedSplit(SplitSelectionStrategy):\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"random_stratified_split\"\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return \"Randomly sample up to PER_CHILD_CAP files per child tag, split 80/20 into global train/test sets, ensuring disjointness.\"\n",
    "\n",
    "    def __init__(self, parents, per_child_cap, train_ratio, random_seed=None):\n",
    "        self.parents = parents\n",
    "        self.per_child_cap = per_child_cap\n",
    "        self.train_ratio = train_ratio\n",
    "        self.random_seed = random_seed or 42\n",
    "\n",
    "    def select_files(self, session: Session):\n",
    "        random.seed(self.random_seed)\n",
    "        train_ids, test_ids = [], []\n",
    "        for parent in self.parents:\n",
    "            for child in list_all_descendants(session, parent):\n",
    "                files = files_with_tag_and_embedding(session, child.label)\n",
    "                if not files:\n",
    "                    continue\n",
    "                cap = min(self.per_child_cap, len(files))\n",
    "                sampled = random.sample(files, cap) if len(files) > cap else list(files)\n",
    "                random.shuffle(sampled)\n",
    "                split_idx = int(len(sampled) * self.train_ratio)\n",
    "                train_ids += [f.id for f in sampled[:split_idx]]\n",
    "                test_ids += [f.id for f in sampled[split_idx:]]\n",
    "\n",
    "        # Deduplicate and enforce disjointness\n",
    "        train_ids = list(dict.fromkeys(train_ids))\n",
    "        test_ids = [fid for fid in dict.fromkeys(test_ids) if fid not in set(train_ids)]\n",
    "        return train_ids, test_ids\n",
    "    \n",
    "class NoOpNeighborFilter(NeighborFilterStrategy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"no_op_filter\"\n",
    "    @property\n",
    "    def description(self):\n",
    "        return \"Does not filter any neighbors; uses all candidates.\"\n",
    "    \n",
    "    def filter(self, candidate_ids, test_file_hash):\n",
    "        return candidate_ids\n",
    "    \n",
    "class MajorityVoteLabeler(LabelingStrategy):\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"majority_vote\"\n",
    "    \n",
    "    @property\n",
    "    def description(self):\n",
    "        return \"Infers label by majority vote among top-k neighbors.\"\n",
    "    \n",
    "    def infer_label(self, neighbor_tags, neighbor_scores):\n",
    "        if not neighbor_tags:\n",
    "            return None\n",
    "            \n",
    "        tag_counts = Counter(neighbor_tags)\n",
    "        most_common_items = tag_counts.most_common(1)\n",
    "        most_common_tag, _ = most_common_items[0]\n",
    "        \n",
    "        return most_common_tag\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_collections_from_strategy(session: Session, strategy: SplitSelectionStrategy):\n",
    "    \"\"\"\n",
    "    Generic builder that uses a SplitSelectionStrategy to populate train/test FileCollections.\n",
    "    The strategy name and description are stored in metadata.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Running split strategy: {strategy.name}\")\n",
    "    train_ids, test_ids = strategy.select_files(session)\n",
    "\n",
    "    # Use the base class method with all required parameters\n",
    "    prov = strategy.assemble_provenance_info(\n",
    "        parents=PARENTS,\n",
    "        embedding_col=EMBED_COL,\n",
    "        split_ratio=TRAIN_RATIO,\n",
    "        per_child_cap=PER_CHILD_CAP,\n",
    "        random_seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    train_coll = create_or_get_collection(session=session,\n",
    "                                          name=f\"{prov.split_strategy}_{prov.embedding_column}_train\",\n",
    "                                          prov=prov)\n",
    "    test_coll  = create_or_get_collection(session=session,\n",
    "                                          name=f\"{prov.split_strategy}_{prov.embedding_column}_test\",\n",
    "                                          prov=prov)\n",
    "\n",
    "    # Get File objects\n",
    "    train_files = session.query(File).filter(File.id.in_(train_ids)).all()\n",
    "    test_files  = session.query(File).filter(File.id.in_(test_ids)).all()\n",
    "\n",
    "    add_members(session, train_coll, train_files, role=\"train\")\n",
    "    add_members(session, test_coll, test_files, role=\"test\")\n",
    "\n",
    "    logger.info(f\"Train: {len(train_files)}, Test: {len(test_files)}\")\n",
    "\n",
    "    return train_coll, test_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a093218",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = get_db_engine()\n",
    "with Session(engine) as session:\n",
    "    split_strategy = RandomStratifiedSplit(\n",
    "        parents=PARENTS,\n",
    "        per_child_cap=PER_CHILD_CAP,\n",
    "        train_ratio=TRAIN_RATIO,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )\n",
    "    neighbor_filter = NoOpNeighborFilter()\n",
    "    labeler = MajorityVoteLabeler()\n",
    "\n",
    "    train_coll, test_coll = build_collections_from_strategy(session, split_strategy)\n",
    "\n",
    "    knn_run = KNNRun(\n",
    "        k=5,\n",
    "        name=\"baseline_random_split\",\n",
    "        description=\"Baseline KNN evaluation using random stratified split and majority voting.\",\n",
    "        training_collection=train_coll,\n",
    "        test_collection=test_coll,\n",
    "    )\n",
    "\n",
    "    print(f\"KNNRun created: {knn_run.name}\")\n",
    "    print(f\"Split: {split_strategy.name}, Filter: {neighbor_filter.name}, Labeler: {labeler.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54e367",
   "metadata": {},
   "source": [
    "The code from this point down is for getting and analyzing knn performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44329887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adankert\\projects\\file_code_tagger\\utils.py:60: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  r\"N:\\PPDO\\Records\"  (Windows)  or  \"/mnt/records\" (Linux).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sqlalchemy.orm import Session\n",
    "from db.db import get_db_engine\n",
    "from db.models import FileCollection, FileCollectionMember, File, FileContent, FileTagLabel\n",
    "from utils import cosine_similarity_batch\n",
    "\n",
    "def get_embeddings_and_tags(session: Session, collection_name: str, embed_col: str = \"minilm_emb\"):\n",
    "    \"\"\"Return (file_id, hash, embedding, tag) tuples for all files in a collection.\"\"\"\n",
    "    emb_col = getattr(FileContent, embed_col)\n",
    "    q = (\n",
    "        session.query(File.id, File.hash, emb_col, FileTagLabel.tag)\n",
    "        .join(FileCollectionMember, File.id == FileCollectionMember.file_id)\n",
    "        .join(FileCollection, FileCollectionMember.collection_id == FileCollection.id)\n",
    "        .join(FileContent, File.hash == FileContent.file_hash)\n",
    "        .join(FileTagLabel, File.id == FileTagLabel.file_id)\n",
    "        .filter(FileCollection.name == collection_name)\n",
    "    )\n",
    "    return q.all()\n",
    "\n",
    "def evaluate_knn(k: int = 5, embed_col: str = \"minilm_emb\"):\n",
    "    engine = get_db_engine()\n",
    "    with Session(engine) as session:\n",
    "        train = get_embeddings_and_tags(session, \"global_train\", embed_col)\n",
    "        test = get_embeddings_and_tags(session, \"global_test\", embed_col)\n",
    "\n",
    "    # convert to arrays for speed\n",
    "    train_vecs = np.stack([np.array(t[2]) for t in train])\n",
    "    train_tags = [t[3] for t in train]\n",
    "    #train_ids  = [t[0] for t in train]\n",
    "\n",
    "    results = []\n",
    "    for fid, fhash, femb, true_tag in test:\n",
    "        femb = np.array(femb)\n",
    "        sims = cosine_similarity_batch(femb, train_vecs)  # shape (N_train,)\n",
    "        topk_idx = np.argsort(sims)[-k:][::-1]  # top k highest cosine similarities\n",
    "        topk_tags = [train_tags[i] for i in topk_idx]\n",
    "        topk_scores = [float(sims[i]) for i in topk_idx]\n",
    "        pred_tag = max(set(topk_tags), key=topk_tags.count)  # majority vote\n",
    "        correct = pred_tag == true_tag\n",
    "        tag_counts = {t: topk_tags.count(t) for t in set(topk_tags)}\n",
    "\n",
    "        results.append({\n",
    "            \"test_file_id\": fid,\n",
    "            \"test_hash\": fhash,\n",
    "            \"true_tag\": true_tag,\n",
    "            \"topk_tags\": topk_tags,\n",
    "            \"topk_scores\": topk_scores,\n",
    "            \"pred_tag\": pred_tag,\n",
    "            \"is_correct\": correct,\n",
    "            \"neighbor_tag_counts\": tag_counts\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dfcafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:16:49,673 db.db INFO Creating database engine\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_knn(k=5)\n",
    "\n",
    "#store in json file\n",
    "with open(\"knn_results.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
