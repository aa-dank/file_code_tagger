{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a846fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from db_models import FilingTag, FileLocation, File\n",
    "from sqlalchemy import create_engine, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def get_db_engine():\n",
    "    \"\"\"Create and return a SQLAlchemy engine for the project database.\"\"\"\n",
    "    conn_string = (\n",
    "        f\"postgresql+psycopg://{os.getenv('PROJECT_DB_USERNAME')}:{os.getenv('PROJECT_DB_PASSWORD')}\"\n",
    "        f\"@{os.getenv('PROJECT_DB_HOST')}:{os.getenv('PROJECT_DB_PORT')}/{os.getenv('PROJECT_DB_NAME')}\"\n",
    "    )\n",
    "    return create_engine(conn_string)\n",
    "\n",
    "# Configure your database session\n",
    "engine = get_db_engine()\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "def file_tag_file_locations(filing_tag):\n",
    "    \"\"\"\n",
    "    Find all FileLocation entries where file_server_directories contains the filing tag label.\n",
    "    \n",
    "    Args:\n",
    "        filing_tag (FilingTag): The filing tag object to search for\n",
    "    \n",
    "    Returns:\n",
    "        list: FileLocation entries matching the criteria\n",
    "    \"\"\"\n",
    "    search_pattern = f\"{filing_tag.label} - \"\n",
    "    \n",
    "    # Query for FileLocation entries where file_server_directories contains the pattern\n",
    "    locations = session.query(FileLocation).filter(\n",
    "        FileLocation.file_server_directories.like(f\"%{search_pattern}%\")\n",
    "    ).all()\n",
    "    \n",
    "    return locations\n",
    "\n",
    "# Get all filing tags\n",
    "all_filing_tags = session.query(FilingTag).all()\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate through all filing tags\n",
    "for tag in all_filing_tags:\n",
    "    # Get matching file locations\n",
    "    locations = file_tag_file_locations(tag)\n",
    "    count = len(locations)\n",
    "    \n",
    "    # Add to results\n",
    "    results.append({\n",
    "        'tag': tag.label,\n",
    "        'description': tag.description,\n",
    "        'file_locations_count': count\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Sort by count in descending order\n",
    "df_results = df_results.sort_values('file_locations_count', ascending=False)\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"Found {len(df_results)} filing tags with {df_results['file_locations_count'].sum()} total file locations\")\n",
    "\n",
    "# Display top tags by location count\n",
    "display(df_results.head(20))\n",
    "\n",
    "# Visualize the top 15 tags by file location count\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_tags = df_results.head(15)\n",
    "sns.barplot(x='file_locations_count', y='tag', data=top_tags)\n",
    "plt.title('Top 15 Filing Tags by File Location Count')\n",
    "plt.xlabel('Number of File Locations')\n",
    "plt.ylabel('Filing Tag')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06eea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spreadsheet from df_results\n",
    "df_results.to_csv('filing_tags_file_locations_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fc8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve database credentials from environment variables\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "\n",
    "# Establish a connection to the database\n",
    "conn = psycopg.connect(\n",
    "    dbname=DB_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    host=DB_HOST\n",
    ")\n",
    "\n",
    "# Create a cursor to execute SQL commands\n",
    "cur = conn.cursor()\n",
    "\n",
    "# SQL statements to create tables\n",
    "sql_commands = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE prototype_runs (\n",
    "      run_id SERIAL PRIMARY KEY,\n",
    "      model_name TEXT NOT NULL,\n",
    "      model_version TEXT NOT NULL,\n",
    "      algorithm TEXT NOT NULL,\n",
    "      hyperparams JSONB,\n",
    "      tag_filter TEXT,\n",
    "      created_at TIMESTAMPTZ DEFAULT now()\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE prototype_members (\n",
    "      run_id INTEGER REFERENCES prototype_runs(run_id) ON DELETE CASCADE,\n",
    "      tag TEXT REFERENCES filing_tags(label),\n",
    "      prototype_id SMALLINT DEFAULT 0,\n",
    "      file_id INTEGER REFERENCES files(id),\n",
    "      PRIMARY KEY (run_id, tag, prototype_id, file_id)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE tag_prototypes ADD COLUMN run_id INTEGER\n",
    "      REFERENCES prototype_runs(run_id) ON DELETE SET NULL;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE prototype_run_metrics (\n",
    "      run_id INTEGER REFERENCES prototype_runs(run_id) ON DELETE CASCADE,\n",
    "      metric_name TEXT,\n",
    "      value NUMERIC,\n",
    "      split TEXT,\n",
    "      PRIMARY KEY (run_id, metric_name, split)\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute each command\n",
    "for command in sql_commands:\n",
    "    cur.execute(command)\n",
    "\n",
    "# Commit changes and close the connection\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "def visualize_directory_tree(\n",
    "    dir_path: Path,\n",
    "    level: int = -1,\n",
    "    limit_to_directories: bool = False,\n",
    "    length_limit: int = 1000000,\n",
    "    exclusion_list: list = None\n",
    "):\n",
    "    \"\"\"Given a directory Path object print a visual tree structure, with optional exclusions.\"\"\"\n",
    "    space = '    '\n",
    "    branch = '│   '\n",
    "    # pointers:\n",
    "    tee = '├── '\n",
    "    last = '└── '\n",
    "\n",
    "    dir_path = Path(dir_path)  # accept string coerceable to Path\n",
    "    files = 0\n",
    "    directories = 0\n",
    "    exclusion_set = set(exclusion_list) if exclusion_list else set()\n",
    "\n",
    "    def inner(dir_path: Path, prefix: str = '', level=-1):\n",
    "        nonlocal files, directories\n",
    "        if not level:\n",
    "            return  # 0, stop iterating\n",
    "        if limit_to_directories:\n",
    "            contents = [d for d in dir_path.iterdir() if d.is_dir() and d.name not in exclusion_set]\n",
    "        else:\n",
    "            contents = [d for d in dir_path.iterdir() if d.name not in exclusion_set]\n",
    "        pointers = [tee] * (len(contents) - 1) + [last] if contents else []\n",
    "        for pointer, path in zip(pointers, contents):\n",
    "            if path.is_dir():\n",
    "                yield prefix + pointer + path.name\n",
    "                directories += 1\n",
    "                extension = branch if pointer == tee else space\n",
    "                yield from inner(path, prefix=prefix + extension, level=level - 1)\n",
    "            elif not limit_to_directories:\n",
    "                yield prefix + pointer + path.name\n",
    "                files += 1\n",
    "\n",
    "    print(dir_path.name)\n",
    "    iterator = inner(dir_path, level=level)\n",
    "    for line in islice(iterator, length_limit):\n",
    "        print(line)\n",
    "    if next(iterator, None):\n",
    "        print(f'... length_limit, {length_limit}, reached, counted:')\n",
    "    print(f'\\n{directories} directories' + (f', {files} files' if files else ''))\n",
    "# exclude irrelevant directories\n",
    "exclude_dirs = [\n",
    "    '.venv',\n",
    "    '__pycache__',\n",
    "    'dev\\__pycache__',\n",
    "    '.git',\n",
    "    'test_files'\n",
    "]\n",
    "\n",
    "visualize_directory_tree(\n",
    "    dir_path= os.getcwd(),\n",
    "    exclusion_list= exclude_dirs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from text_extraction.pdf_extraction import PDFTextExtractor\n",
    "\n",
    "def test_pdf_text_extraction(directory_path):\n",
    "    \"\"\"\n",
    "    Test PDF text extraction on all files in the given directory.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing PDF files.\n",
    "    \"\"\"\n",
    "    extractor = PDFTextExtractor()\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    print(f\"Extracting text from: {file_path}\")\n",
    "                    text = extractor(file_path)\n",
    "                    print(f\"Extracted text (first 500 characters):\\n{text[:500]}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to extract text from {file_path}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "test_directory = \"path/to/your/pdf/directory\"\n",
    "test_pdf_text_extraction(test_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02122255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is for populating /test_files with test files from the server\n",
    "import os\n",
    "import shutil\n",
    "from db_models import FileLocation, File, get_db_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import func, or_\n",
    "\n",
    "SERVER_MOUNT_LOCATION = r\"N:\\PPDO\\Records\"\n",
    "\n",
    "ext_lists_list = [\n",
    "['pdf'],\n",
    "[\"html\", \"htm\", \"mhtml\", \"mht\"],\n",
    "[\"eml\", \"msg\"],\n",
    "[\"png\", \"jpg\", \"jpeg\", \"tif\", \"tiff\", \"bmp\", \"gif\"],\n",
    "[\"docx\", \"docm\", \"doc\", \"rtf\"],\n",
    "[\"xlsx\", \"xlsm\", \"xls\", \"xlsb\", \"ods\", \"csv\", \"tsv\"],\n",
    "['txt', 'md', 'log', 'csv', 'json', 'xml', 'yaml', 'yml', 'ini', 'cfg', 'conf']\n",
    "]\n",
    "\n",
    "def get_test_file_locations(session, n, size_limit, ext_list):\n",
    "    \"\"\"\n",
    "    Get random test files from the database based on specified criteria.\n",
    "    \"\"\"\n",
    "    query = (\n",
    "        session\n",
    "        .query(FileLocation)\n",
    "        .join(File, FileLocation.file)\n",
    "        .filter(\n",
    "            File.size < size_limit,\n",
    "            FileLocation.file_server_directories.isnot(None)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if ext_list:\n",
    "        ext_filters = [\n",
    "            FileLocation.filename.ilike(f\"%.{ext}\") \n",
    "            for ext in ext_list\n",
    "        ]\n",
    "        query = (\n",
    "            query\n",
    "            .filter(or_(*ext_filters))\n",
    "            .order_by(func.random())\n",
    "        )\n",
    "\n",
    "    return query.limit(n).all()\n",
    "\n",
    "def save_test_files_to_directory(session, n, size_limit, ext_lists_list, destintation = os.path.join(os.getcwd(), \"test_files\")):\n",
    "    \"\"\"\n",
    "    Save test files to a specified directory based on given criteria.\n",
    "    \n",
    "    Args:\n",
    "        session: SQLAlchemy session object.\n",
    "        n (int): Number of files to retrieve.\n",
    "        size_limit (int): Maximum file size in bytes.\n",
    "        ext_lists_list: List of lists containing file extensions.\n",
    "        base_dir (str): Base directory to save the test files.\n",
    "    \"\"\"\n",
    "    for ext_list in ext_lists_list:\n",
    "        locations = get_test_file_locations(session, n, size_limit, ext_list)\n",
    "        for loc in locations:\n",
    "            file_path = loc.local_filepath(SERVER_MOUNT_LOCATION)\n",
    "            if file_path and file_path.exists():\n",
    "                dest_path = os.path.join(destintation, loc.filename)\n",
    "                shutil.copy2(file_path, dest_path)\n",
    "                print(f\"Copied {loc.filename} to {dest_path}\")\n",
    "                continue\n",
    "\n",
    "            if file_path and not file_path.exists():\n",
    "                print(f\"File {file_path} does not exist, skipping.\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Test files saved to {destintation}\")\n",
    "\n",
    "\n",
    "size_limit = 150 * 1024 * 1024  # 150 MB\n",
    "n = 15  # Number of files per extension\n",
    "destintation = os.path.join(os.getcwd(), \"test_files\")\n",
    "session = sessionmaker(bind=get_db_engine())()\n",
    "\n",
    "save_test_files_to_directory(\n",
    "    session=session,\n",
    "    n=n,\n",
    "    size_limit=size_limit,\n",
    "    ext_lists_list=ext_lists_list,\n",
    "    destintation=destintation\n",
    ")           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import traceback \n",
    "import tempfile\n",
    "from text_extraction.pdf_extraction import PDFTextExtractor\n",
    "from text_extraction.basic_extraction import TextFileTextExtractor, get_extractor_for_file\n",
    "from text_extraction.image_extraction import ImageTextExtractor\n",
    "from text_extraction.office_doc_extraction import PresentationTextExtractor, SpreadsheetTextExtractor, WordFileTextExtractor\n",
    "from text_extraction.web_extraction import HtmlTextExtractor, EmailTextExtractor\n",
    "from text_extraction.extraction_utils import common_char_replacements, strip_diacritics, normalize_unicode\n",
    "from logging_setups import setup_logger\n",
    "\n",
    "setup_logger(name=\"NotebookLogger\", notebook=True, level=logging.DEBUG)\n",
    "\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Initialize extractors for different file types\n",
    "pdf_extractor = PDFTextExtractor()\n",
    "txt_extractor = TextFileTextExtractor()\n",
    "image_extractor = ImageTextExtractor()\n",
    "presentation_extractor = PresentationTextExtractor()\n",
    "spreadsheet_extractor = SpreadsheetTextExtractor()\n",
    "word_extractor = WordFileTextExtractor()\n",
    "html_extractor = HtmlTextExtractor()\n",
    "email_extractor = EmailTextExtractor()\n",
    "\n",
    "extractors_list = [\n",
    "    pdf_extractor,\n",
    "    txt_extractor,\n",
    "    image_extractor,\n",
    "    presentation_extractor,\n",
    "    spreadsheet_extractor,\n",
    "    word_extractor,\n",
    "    html_extractor,\n",
    "    email_extractor\n",
    "]\n",
    "display_text_len = 800\n",
    "test_extraction_path = os.path.join(os.getcwd(), \"test_files\")\n",
    "\n",
    "for file in os.listdir(test_extraction_path):\n",
    "    file_path = os.path.join(test_extraction_path, file)\n",
    "    def text_assessment(extracted_text):\n",
    "        if not extracted_text:\n",
    "            print(f\"No text extracted from {file} in temporary directory\")\n",
    "        elif len(extracted_text) > display_text_len:\n",
    "            print(f\"Extracted text (first {display_text_len} characters) from {file}:\\n{extracted_text[:display_text_len]}\\n\")\n",
    "        else:\n",
    "            print(f\"Extracted text from {file}:\\n{extracted_text}\\n\")\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"Skipping {file}, not a file.\")\n",
    "        continue\n",
    "    \n",
    "    extractor = get_extractor_for_file(file_path=file_path, extractors=extractors_list)\n",
    "    if extractor:\n",
    "        try:\n",
    "            print(f\"Extracting text from {file} using {extractor.__class__.__name__}\")\n",
    "            text = extractor(file_path)\n",
    "            text_assessment(text)\n",
    "        \n",
    "        except PermissionError as pe:\n",
    "            # Handle PermissionError by copying the file to a temporary directory\n",
    "            temp_dir = tempfile.gettempdir()\n",
    "            temp_file_path = os.path.join(temp_dir, file)\n",
    "            shutil.copy2(file_path, temp_file_path)\n",
    "            print(f\"Copied {file} to temporary directory: {temp_file_path}\")\n",
    "            \n",
    "            # Retry extraction from the temporary file\n",
    "            try:\n",
    "                text = extractor(temp_file_path)\n",
    "                text = common_char_replacements(text)\n",
    "                text = strip_diacritics(text)\n",
    "                text = normalize_unicode(text)\n",
    "                text_assessment(text)\n",
    "\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Failed to extract text from {file} in temporary directory: {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Failed to extract text from {file}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(f\"No suitable extractor found for {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is for removing erronious tag labels from the database\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from db_models import FileTagLabel, get_db_engine\n",
    "\n",
    "def delete_file_tag_labels_by_hash(session, file_hash: str) -> int:\n",
    "    \"\"\"\n",
    "    Delete FileTagLabel entries associated with the given file hash.\n",
    "    Returns the number of rows deleted.\n",
    "    \"\"\"\n",
    "    deleted_count = (\n",
    "        session.query(FileTagLabel)\n",
    "               .filter(FileTagLabel.file_hash == file_hash)\n",
    "               .delete(synchronize_session=False)\n",
    "    )\n",
    "    session.commit()\n",
    "    return deleted_count\n",
    "\n",
    "hsh_lst = [\n",
    "    \"12e09752a751819a10d2357cbd72cc5104b06f8c\",\n",
    "    \"9055a3c243f23f27ea82cc10046db2ed9aba401d\",\n",
    "    \"6007b740d71580b20ae7a4d6b655af7b26a015be\",\n",
    "    \"a0a13159ecdf28f465f330a21c0f4dad247def2b\",\n",
    "    \"c8b1012ca42876d3a21f6dad2ca3c65bb325708a\"\n",
    "]\n",
    "\n",
    "\n",
    "with sessionmaker(bind=get_db_engine())() as session:\n",
    "    for hsh in hsh_lst:\n",
    "        deleted_count = delete_file_tag_labels_by_hash(session, hsh)\n",
    "        print(f\"Deleted {deleted_count} FileTagLabel entries for hash {hsh}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e8d664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 1 FileEmbedding entries for hash 12e09752a751819a10d2357cbd72cc5104b06f8c.\n",
      "Deleted 1 FileEmbedding entries for hash 9055a3c243f23f27ea82cc10046db2ed9aba401d.\n",
      "Deleted 1 FileEmbedding entries for hash 6007b740d71580b20ae7a4d6b655af7b26a015be.\n",
      "Deleted 1 FileEmbedding entries for hash a0a13159ecdf28f465f330a21c0f4dad247def2b.\n",
      "Deleted 1 FileEmbedding entries for hash c8b1012ca42876d3a21f6dad2ca3c65bb325708a.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "from db_models import FileEmbedding, get_db_engine\n",
    "\n",
    "def delete_file_embeddings_by_hash(session, file_hash: str) -> int:\n",
    "    \"\"\"\n",
    "    Delete FileEmbedding entries associated with the given file hash.\n",
    "    Returns the number of rows deleted.\n",
    "    \"\"\"\n",
    "    deleted_count = (\n",
    "        session.query(FileEmbedding)\n",
    "               .filter(FileEmbedding.file_hash == file_hash)\n",
    "               .delete(synchronize_session=False)\n",
    "    )\n",
    "    session.commit()\n",
    "    return deleted_count\n",
    "\n",
    "\n",
    "hsh_lst = [\n",
    "    \"12e09752a751819a10d2357cbd72cc5104b06f8c\",\n",
    "    \"9055a3c243f23f27ea82cc10046db2ed9aba401d\",\n",
    "    \"6007b740d71580b20ae7a4d6b655af7b26a015be\",\n",
    "    \"a0a13159ecdf28f465f330a21c0f4dad247def2b\",\n",
    "    \"c8b1012ca42876d3a21f6dad2ca3c65bb325708a\"\n",
    "]\n",
    "\n",
    "with sessionmaker(bind=get_db_engine())() as session:\n",
    "    for hsh in hsh_lst:\n",
    "        deleted_embeddings = delete_file_embeddings_by_hash(session, hsh)\n",
    "        print(f\"Deleted {deleted_embeddings} FileEmbedding entries for hash {hsh}.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
